{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import csv\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Solving environment: Out of memory allocating 18446744071562067968*4 bytes!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} numpy matplotlib pandas nltk word2vec gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTwitterdata():\n",
    "    '''# Load  \"Twitter US Airline Sentiment\". It can be found on kaggle \n",
    "    https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment\n",
    "    and contains ~15000 tweets on airline-relevant topics from 2015. \n",
    "    '''\n",
    "\n",
    "    def load_data(filename):\n",
    "        rtn = []\n",
    "        #open the file with the csv reader\n",
    "        with open(filename, newline='') as csvfile:\n",
    "            tweets = csv.reader(csvfile, delimiter=',', quotechar='\"') \n",
    "            next(tweets)#skip the header   \n",
    "            for row in tweets:\n",
    "                rtn.append(row[10])\n",
    "        return rtn\n",
    "    tweets = load_data('Tweets.csv')\n",
    "    #figure out how much data we have\n",
    "    size = sum([i.__sizeof__() for i in tweets]) + tweets.__sizeof__()\n",
    "    print('Number: ', len(tweets), '\\t Size:', size/1e6,'MB','\\t Bytes per tweet:', size/len(tweets))\n",
    "    return(tweets)\n",
    "    \n",
    "tweets = loadTwitterdata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "# the word vectors, a 3M x 300 matrix, must be downloaded.\n",
    "import gensim.downloader\n",
    "# Show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object-oriented everything, we must create an instance of the word2vec engine\n",
    "wv = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to https://stackoverflow.com/questions/35596031/gensim-word2vec-find-number-of-words-in-vocabulary \n",
    "# wv acts like a dictionary. \n",
    "len(wv)  # gives me the number of items in vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv[\"Chicago\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wv[\"Chicago\"].reshape((300,1)).T )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meh, not the most brilliant visualization here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(wv[\"Chicago\"], bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a suggestion from google AI, start with a list of words, create a list of vectors, and\n",
    "# convert both into a dataframe (which will handle the labels more nicely)\n",
    "words = ['Chicago', \"Boston\", \"Philadelphia\", \"lawyer\", \"attorney\", \"cook\", \"waiter\", \"thylacine\", \"wallaby\"]\n",
    "vectors = [wv[word] for word in words]\n",
    "df = pd.DataFrame(vectors, index=words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(df.to_numpy() , aspect=\"auto\", interpolation=\"nearest\") #, ylabels=words)\n",
    "plt.yticks(range(len(words)), words)\n",
    "plt.xlim([0,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.clustermap(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google code suggestion for \"all-against-all distances pandas dataframe of vectors\"\n",
    "# clearly cdist does the n^2 / 2 distance calculations, and DataFrame just makes it pretty.\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def pairwise_distances(df, metric='euclidean'):\n",
    "    \"\"\"Calculate pairwise distances for a DataFrame of vectors.\"\"\"\n",
    "    return pd.DataFrame(cdist(df.values, df.values, metric=metric), index=df.index, columns=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pairwise_distances(df)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(d , aspect=\"auto\", interpolation=\"nearest\")\n",
    "\n",
    "plt.yticks(range(len(words)), words)\n",
    "plt.xticks(range(len(words)), words, rotation = 45)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, this demo has wv.most_similar.. which takes a \"positive\" and a \"negative\" list of words\n",
    "# https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html\n",
    "print(wv.most_similar(positive=['mushroom'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.most_similar(positive=['lawyers', \"thylacine\"], negative=[\"lawyer\"], topn=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.most_similar(positive=[wv[\"air\"]/2 + wv[\"water\"] / 2 ], topn=4))\n",
    "print(wv.most_similar(positive=[wv[\"air\"]/2 + wv[\"fire\"] / 2 ], topn=4))\n",
    "print(wv.most_similar(positive=[wv[\"air\"]/2 + wv[\"earth\"] / 2 ], topn=4))\n",
    "print(wv.most_similar(positive=[wv[\"water\"]/2 + wv[\"earth\"] / 2 ], topn=4))\n",
    "print(wv.most_similar(positive=[wv[\"fire\"]/2 + wv[\"earth\"] / 2 ], topn=4))\n",
    "print(wv.most_similar(positive=[wv[\"water\"]/2 + wv[\"fire\"] / 2 ], topn=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.most_similar(positive=['air', 'water'], topn=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just average everything together.. (This isn't going to bite us later, is it?)\n",
    "def string_to_vec(s):\n",
    "    N=300\n",
    "    n=0\n",
    "    t = np.zeros(300)\n",
    "    tok1 = nltk.word_tokenize(s)\n",
    "    for token in tok1:\n",
    "        try:\n",
    "            c = wv[token]\n",
    "            t = t+c\n",
    "            n = n+1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return(t / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twittervectors = [string_to_vec(tweet) for tweet in tweets] \n",
    "twittervectors_small = [string_to_vec(tweet) for tweet in tweets[0:200]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitframe_small  = pd.DataFrame(twittervectors_small, index=tweets[0:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(twitframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit_d = pairwise_distances(twitframe_small)\n",
    "sns.clustermap(twit_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(twit_d).reshape(-1), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.most_similar(positive=['disappointment'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, Cookie, which one of these things is not like the others?\n",
    "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (wv.most_similar(positive=[\"Merced\", \"State\"], negative=[\"City\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (wv.most_similar(positive=[\"Merced\", \"Chicago\"], negative=[\"Illinois\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (wv.most_similar(positive=[\"Chicago\", \"State\"], negative=[\"City\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = wv['computer']  # Get word2vec number for a word \n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(vector, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
