{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Python Document Search Engine\n",
    "\n",
    "Now, we will start to put together all of the topics that we have studied so far into a series of \"Python Recipes\"---coding examples that illustrate the power of thinking hard about how data is organized and structured. In the first example, we will consider a \"Python Search Engine\" that will identify relevant items given a query string.\n",
    "\n",
    "We're going to start with a dataset of tweets about airlines called \"Twitter US Airline Sentiment\".  It can be found on kaggle https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment.  The data is from February 2015, and was described by crowdflower as \"A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as “late flight” or “rude service”).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "def load_data(filename):\n",
    "    rtn = []\n",
    "    #open the file with the csv reader\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        tweets = csv.reader(csvfile, delimiter=',', quotechar='\"') \n",
    "        next(tweets)#skip the header   \n",
    "        for row in tweets:\n",
    "            rtn.append(row[10])\n",
    "    return rtn\n",
    "    \n",
    "\n",
    "tweets = load_data('Tweets.csv')\n",
    "\n",
    "#figure out how much data we have\n",
    "size = sum([i.__sizeof__() for i in tweets]) + tweets.__sizeof__()\n",
    "\n",
    "print('Number: ', len(tweets), '\\t Size:', size/1e6,'MB','\\t Bytes per tweet:', size/len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe some trial and error needed to instal the 400Mb data in the right\n",
    "# environment. \n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets[0:15]:\n",
    "    print(tweet[0:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "d = np.zeros(shape =(len(tweets), len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX = 1000\n",
    "nlptweets = [ nlp(t) for t in tweets[0:MAX]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlptweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nlptweets[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(nlptweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,t in enumerate(nlptweets[0:MAX]):\n",
    "#    if i%100 == 0 : \n",
    "#        print (i)\n",
    "    for j,t2 in enumerate(nlptweets[0:MAX]):\n",
    "        d[i,j] = t.similarity(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,t in enumerate(nlptweets[0:MAX]):\n",
    "    for j,t2 in enumerate(nlptweets[0:MAX]):\n",
    "        d[i,j] = t.similarity(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "MAX = 25\n",
    "plt.imshow(d[0:MAX, 0:MAX], aspect=\"auto\", interpolation=\"nearest\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smalld = d[0:MAX, 0:MAX] \n",
    "import scipy\n",
    "import seaborn as sns\n",
    "clusters = sns.clustermap(smalld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -y seaborn scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(smalld.reshape(-1), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(clusters.dendrogram_row.reordered_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered = [nlptweets[x] for x in list(clusters.dendrogram_row.reordered_ind)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(MAX):\n",
    "    print (reordered[i][0:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(tweets):\n",
    "    idx = {}\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        for word in tweet.split():\n",
    "            if word in idx.keys():\n",
    "                idx[word].add(i)\n",
    "            else:\n",
    "                idx[word] = set([i])\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = build_index(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index.keys()),len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(index.keys())[0:15]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index['I'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that build_index is about a 100x slower than a single query. What does this mean? Basically, indexing is only valuable if you run a lot of queries! \n",
    "\n",
    "The next challenge is how to use an inverted index to answer general substring queries. In class, we showed how to do exact keyword lookup but the phrase 'choppy landing' is actually two words. This is actually not a problem, and we can use the inverted index to retrieve a set of candidates and then use the naive find method among just those candidates. <font color = red> Is this a good idea? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's write a new find function that can use this index:\n",
    "* It splits the phrase into its constituent words\n",
    "* Searches each word in the inverted index, finds a set of possibly relevant tweets (that match on a single word)\n",
    "* Then double checks that set.\n",
    "<font color=red>What kind of imperfection in the index are we working around here, false positives or false negatives?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def find(phrase, tweets):\n",
    "    #Naive full scan approach\n",
    "    start = datetime.datetime.now()\n",
    "    rtn = []    \n",
    "    for t in tweets:\n",
    "        if phrase in t:\n",
    "            rtn.append(t)\n",
    "    print('Find() elapsed time: ', (datetime.datetime.now()-start).total_seconds())          \n",
    "    return rtn\n",
    "\n",
    "\n",
    "find('choppy landing', tweets)\n",
    "\n",
    "find('LAX', tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(phrase, tweets, index):\n",
    "    start = datetime.datetime.now()\n",
    "    words = phrase.split()\n",
    "    #find tweets that contain all words\n",
    "    candidates = None\n",
    "    \n",
    "    for w in words: #for each words in the phrase\n",
    "        try:\n",
    "            if candidates is None:\n",
    "                candidates = index[w] #return the set of tweets for w\n",
    "            else:\n",
    "                candidates = candidates.intersection(index[w])\n",
    "        except KeyError:\n",
    "            return []\n",
    "    \n",
    "    candidate_tweets = [tweets[ref] for ref in candidates]\n",
    "    print(\"number of candidates\", len(candidate_tweets)) \n",
    "    return find(phrase, candidate_tweets)\n",
    "    print('find_index() elapsed time: ', (datetime.datetime.now()-start).total_seconds())\n",
    "    \n",
    "find_index('choppy landing', tweets, index)\n",
    "#find_index('LAX', tweets, index)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(find_index('to Chicago', tweets, index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(find_index('here again', tweets, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bug here; the index used .translate() to remove punctuation from the tokens.  What kind of error will this mismatch between the query and the canonical form of the token in the index provoke?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, you are paying a small upfront cost for greatly improved find performance (nearly a 1000x faster!). Speed is only aspect of search engine performance. We also like to support situations where a user mistypes a phrase. For example, if we mistype choppy landing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_index('chopy landing', tweets, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "len(index.keys())\n",
    "for key in index.keys():\n",
    "    if re.search(\"^ch.*y\", key):\n",
    "        print(key, len(index[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our system returns nothing. This is less than optimal.  How many approaches can we think of to handle mispellings?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did you mean? \n",
    "So now we are going to write a utility that can identify mispelling and typos and suggest potential alternatives. So let's start off with a naive approach that simply finds the closest word in the index in terms of edit distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distance\n",
    "\n",
    "print( \"Jaccard('a b', 'b c')=\", distance.jaccard('a b', 'b c'))\n",
    "print( \"Levenshtein('a b', 'b c')=\", distance.levenshtein('a b','b c') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def did_you_mean_naive(word, index):\n",
    "    start = datetime.datetime.now()\n",
    "    if word in index:\n",
    "        return word\n",
    "    else:\n",
    "        distances = [(distance.levenshtein(word, iw), iw) for iw in index]\n",
    "        distances.sort()\n",
    "        print('did_you_mean_naive() elapsed time: ', (datetime.datetime.now()-start).total_seconds())\n",
    "        return distances[0][1]\n",
    "\n",
    "did_you_mean_naive('chopy', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "did_you_mean_naive('discont', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "did_you_mean_naive('dtg', index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The suggestion utility runs much slower than the actual query!!! How do we fix this? We can use the same trick as before: a fast algorithm to find reasonable candidates and a slower algorithm to refine those candidates.\n",
    "\n",
    "In fact, we will use an inverted index again. Just this time over sub-sequences of letters and not words. The first thing that we are going to do is to calculate n-grams these are contiguous sub-sequences of letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram\n",
    "#def find_ngrams(word, n):\n",
    "#    return list(zip(*[word[i:] for i in range(n)]))\n",
    "\n",
    "def find_ngrams(word, n):\n",
    "    '''digest a word (a string) into a list of len(word)-n+1 \n",
    "    ngrams of length n.'''\n",
    "    return [word[i:i+n] for i in range(0, len(word)-n+1)]\n",
    "\n",
    "find_ngrams('dave', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to build a \"word\" index, an indexing structure that maps ngrams to words that contain them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_index(index, n):\n",
    "    '''Builds a dictionary that maps ngrams contained in the \n",
    "    keys of index to the keys themselves.'''\n",
    "    start = datetime.datetime.now()  \n",
    "    word_index = {}\n",
    "    for word in index:\n",
    "        ngrams = find_ngrams(word, n)     \n",
    "        for subseq in ngrams:       \n",
    "            if subseq not in word_index:\n",
    "                word_index[subseq] = set()       \n",
    "            word_index[subseq].add(word) #add a pointer to the relevant word\n",
    "    \n",
    "    print('build_word_index() elapsed time: ', (datetime.datetime.now()-start).total_seconds())\n",
    "    return word_index\n",
    "\n",
    "word_index = build_word_index(index, 3)\n",
    "print(repr(word_index)[0:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this word index to build a more sophisticated search:\n",
    "* Only consider words that share a minimum number of ngrams with the lookup word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def did_you_mean_better(word, word_index, n, thresh=1):\n",
    "    '''Finds the closest key in index to the query word, but only check\n",
    "    for words that share at least one ngram with the query word.  Uses\n",
    "    word_index.\n",
    "    '''\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    candidate_words = {}\n",
    "    ngrams = find_ngrams(word, n)\n",
    "    \n",
    "    for ngram in ngrams:\n",
    "        candidates = word_index.get(ngram, set())\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            candidate_words[candidate] = candidate_words.get(candidate,0) + 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    distances = [(distance.levenshtein(word, iw), iw) for iw in candidate_words \n",
    "                 if candidate_words[iw] >= thresh]\n",
    "    distances.sort()\n",
    "        \n",
    "    print('did_you_mean_better() elapsed time: ', (datetime.datetime.now()-start).total_seconds())\n",
    "        \n",
    "    return distances[0][1]\n",
    "    \n",
    "\n",
    "did_you_mean_better('chopy', word_index, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "did_you_mean_better('Chicgo', word_index, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "did_you_mean_better('diner', word_index, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how much faster this approach is!! 0.992237 secs v.s. 0.003581 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now, let's write the full program and try out some queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_final(phrase, \\\n",
    "               tweets, \\\n",
    "               index, \\\n",
    "               word_index, \\\n",
    "               n=3, \\\n",
    "               thresh=1):\n",
    "    print('Searching for...' + phrase + \" in \" + str(len(tweets)) + \" tweets\")\n",
    "    out = find_index(phrase, tweets, index)\n",
    "    print('Found ' + str(len(out)) + ' matches')\n",
    "    \n",
    "    if len(out) == 0:\n",
    "        for word in phrase.split():\n",
    "            if word not in index:\n",
    "                print('Did you mean: ' + did_you_mean_better(word, word_index, n, thresh) + ' instead of ' + word + '?')\n",
    "    else:\n",
    "        print(out)\n",
    "\n",
    "find_final('choppy landing', tweets, index, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_final('chopy landing', tweets, index, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_final('choppy landig', tweets, index, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_final('LAX', tweets, index, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_final('LAXS', tweets, index, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
